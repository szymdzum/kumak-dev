---
title: "Adding llms.txt to Your Astro Blog"
description: "How to implement the llms.txt standard in Astro without dependencies. Three endpoints, ~150 lines of TypeScript, and your content becomes agent-accessible."
pubDate: 2025-11-30
category: "tutorial"
tags: ["astro", "llms-txt", "ai", "developer-tools"]
keywords:
  [
    "llms.txt",
    "Astro",
    "AI agents",
    "content accessibility",
    "static site",
    "TypeScript",
  ]
author: "Szymon Dzumak"
showToc: true
featured: false
externalLinks:
  - title: "llms.txt Specification"
    url: "https://llmstxt.org/"
  - title: "Complete llms.ts (Gist)"
    url: "https://gist.github.com/szymdzum/a6db6ff5feb0c566cbd852e10c0ab0af"
  - title: "This site's llms.txt"
    url: "https://kumak.dev/llms.txt"
  - title: "Full content version"
    url: "https://kumak.dev/llms-full.txt"
---

import TldrBox from '@components/TldrBox.astro';

<TldrBox>
llms.txt is a proposed standard for making websites readable by AI agents. For an Astro blog, you need three endpoints: an index (`/llms.txt`), full content (`/llms-full.txt`), and per-post files (`/llms/[slug].txt`). Skip the npm packages. The implementation is ~150 lines of TypeScript with zero dependencies.
</TldrBox>

The [llms.txt specification](https://llmstxt.org/) proposes a standard location for LLM-readable content, similar to `robots.txt` for crawlers or `sitemap.xml` for search engines. The idea: provide a clean, structured text file that AI agents can fetch to understand your site's content without parsing HTML.

For a blog, this means agents can read your posts directly, without scraping, without dealing with navigation chrome, without parsing markup.

## What You're Building

Three endpoints:

- `/llms.txt` — Index with links to all posts
- `/llms-full.txt` — Complete content in one file
- `/llms/[slug].txt` — Individual post content

The index is lightweight. Agents fetch it first, see what's available, then request specific posts or the full dump depending on their needs.

## The Core Builder

The entire implementation shares one document builder. It takes sections, flattens them, normalizes whitespace, and returns a text response:

```typescript
// src/utils/llms.ts

function doc(...sections: (string | string[])[]): Response {
  const content = sections
    .flat()
    .join("\n")
    .replace(/\n{3,}/g, "\n\n")
    .trim();

  return new Response(content + "\n", {
    headers: { "Content-Type": "text/plain; charset=utf-8" },
  });
}
```

Every endpoint uses this. No inconsistent joining patterns.

## Shared Helpers

Three small functions handle the repeating patterns:

```typescript
function formatDate(date: Date): string {
  return date.toISOString().split("T")[0];
}

function header(name: string, description: string): string[] {
  return [`# ${name}`, "", `> ${description}`];
}

function linkList(title: string, items: LlmsItem[], site: string): string[] {
  return [
    "",
    `## ${title}`,
    ...items.map((item) => `- [${item.title}](${site}${item.link}): ${item.description}`),
  ];
}

function postMeta(site: string, link: string, pubDate: Date, category: string): string[] {
  return [`URL: ${site}${link}`, `Published: ${formatDate(pubDate)}`, `Category: ${category}`];
}
```

## Stripping MDX Syntax

If you use MDX, your post bodies contain import statements and JSX components. Agents don't need these:

```typescript
const MDX_PATTERNS = [
  /^import\s+.+from\s+['"].+['"];?\s*$/gm,
  /<[A-Z][a-zA-Z]*[^>]*>[\s\S]*?<\/[A-Z][a-zA-Z]*>/g,
  /<[A-Z][a-zA-Z]*[^>]*\/>/g,
] as const;

function stripMdx(content: string): string {
  return MDX_PATTERNS.reduce((text, pattern) => text.replace(pattern, ""), content).trim();
}
```

This removes `import` lines and any PascalCase components (the convention for JSX). Standard markdown passes through unchanged.

## The Three Generators

With the helpers in place, each generator is minimal:

```typescript
export function llmsTxt(config: LlmsTxtConfig): Response {
  const sections = [
    header(config.name, config.description),
    linkList("Posts", config.items, config.site),
  ];

  if (config.optional?.length) {
    sections.push(linkList("Optional", config.optional, config.site));
  }

  return doc(...sections);
}

export function llmsFullTxt(config: LlmsFullTxtConfig): Response {
  const head = [
    ...header(config.name, config.description),
    "",
    `Author: ${config.author}`,
    `Site: ${config.site}`,
    "",
    "---",
  ];

  const posts = config.items.flatMap((item) => [
    "",
    `## ${item.title}`,
    "",
    ...postMeta(config.site, item.link, item.pubDate, item.category),
    "",
    `> ${item.description}`,
    "",
    stripMdx(item.body),
    "",
    "---",
  ]);

  return doc(head, posts);
}

export function llmsPost(config: LlmsPostConfig): Response {
  const { post, site, link } = config;
  const { title, description, pubDate, category } = post.data;

  return doc(
    `# ${title}`,
    "",
    `> ${description}`,
    "",
    ...postMeta(site, link, pubDate, category),
    "",
    stripMdx(post.body ?? ""),
  );
}
```

## The Endpoints

With utilities in place, the endpoints are minimal. The index:

```typescript
// src/pages/llms.txt.ts
import type { APIRoute } from "astro";
import { siteConfig } from "@/site-config";
import { llmsTxt, postsToLlmsItems } from "@utils/llms";
import { getAllPosts } from "@utils/posts";

export const GET: APIRoute = async () => {
  const posts = await getAllPosts();

  return llmsTxt({
    name: siteConfig.name,
    description: siteConfig.description,
    site: siteConfig.url,
    items: postsToLlmsItems(posts, (slug) => `/llms/${slug}.txt`),
    optional: [
      { title: "About", link: "/about", description: "About the author" },
      { title: "Full Content", link: "/llms-full.txt", description: "All posts in one file" },
    ],
  });
};
```

The full content dump:

```typescript
// src/pages/llms-full.txt.ts
import type { APIRoute } from "astro";
import { siteConfig } from "@/site-config";
import { llmsFullTxt, postsToLlmsFullItems } from "@utils/llms";
import { getAllPosts } from "@utils/posts";

export const GET: APIRoute = async () => {
  const posts = await getAllPosts();

  return llmsFullTxt({
    name: siteConfig.name,
    description: siteConfig.description,
    author: siteConfig.author,
    site: siteConfig.url,
    items: postsToLlmsFullItems(posts, (slug) => `/${slug}`),
  });
};
```

Per-post endpoints using dynamic routes:

```typescript
// src/pages/llms/[slug].txt.ts
import type { GetStaticPaths } from "astro";
import { siteConfig } from "@/site-config";
import { llmsPost } from "@utils/llms";
import { getAllPosts, type BlogPost } from "@utils/posts";

export const getStaticPaths: GetStaticPaths = async () => {
  const posts = await getAllPosts();
  return posts.map((post) => ({
    params: { slug: post.slug },
    props: { post },
  }));
};

export const GET = ({ props }: { props: { post: BlogPost } }) => {
  return llmsPost({
    post: props.post,
    site: siteConfig.url,
    link: `/${props.post.slug}`,
  });
};
```

## Discovery

Agents need to find your llms.txt. Add a link tag to your HTML head, similar to RSS discovery:

```html
<link rel="alternate" type="text/plain" href="/llms.txt" title="LLMs.txt" />
```

This isn't part of the spec, but it follows web conventions. The spec itself relies on the well-known path (`/llms.txt`) and community directories like [llmstxt.site](https://llmstxt.site).

## Limitations

This approach works for content collections with markdown or MDX bodies. The implementation reads `post.body` directly, which is raw text.

For component-based pages (React, Vue, Svelte, or plain `.astro` files), there's no markdown body to extract. You'd need a different strategy: render to HTML and strip tags (lossy), maintain separate content files (duplicate effort), or use a headless CMS where content exists independently of components.

For most blogs, content collections are the right choice anyway. If your site is component-heavy, consider whether those pages even belong in llms.txt.

## Why Not Use a Library?

There are Astro integrations for llms.txt. They auto-generate from all pages at build time. This sounds convenient, but:

1. You get everything, including pages you might not want exposed
2. No per-post endpoints
3. No control over the output format
4. Another dependency to maintain

The implementation above is ~150 lines of TypeScript. You control exactly what's included. You can add per-post endpoints. You can customise the format. And you understand every line.

For something this simple, the DIY approach wins.

## The Result

After deploying:

- `/llms.txt` lists all posts with descriptions
- `/llms-full.txt` contains everything for agents that want the full context
- `/llms/post-slug.txt` serves individual posts

Agents can fetch the index, pick what they need, and get clean markdown without parsing your site's HTML. That's the point of the standard: make your content accessible to the tools people are actually using.
